{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "effa287f",
   "metadata": {},
   "source": [
    "#### 1. 项目环境配置与全局参数\n",
    "定义项目所需的依赖库、硬件设备配置、文件路径及超参数。\n",
    "若不需要重新训练模型，可直接运行单元格1，2，3，4，7。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94cb63bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "运行环境: cuda | 模型架构: swinv2_tiny_window16_256\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import hashlib\n",
    "import jso\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision import datasets, transforms\n",
    "import timm\n",
    "\n",
    "# 忽略非必要的警告信息\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class Config:\n",
    "    # --- 路径配置 (相对路径) ---\n",
    "    BASE_DIR = Path.cwd()\n",
    "    DATA_DIR = BASE_DIR / \"data\"              # 原始数据集目录\n",
    "    BACKUP_DIR = BASE_DIR / \"data_backup\"     # 备份目录 (存放重复文件)\n",
    "    SAVE_DIR = BASE_DIR / \"checkpoints\"       # 模型权重保存目录\n",
    "    \n",
    "    # --- 训练超参数 ---\n",
    "    model_name = 'swinv2_tiny_window16_256'   # 选用模型架构\n",
    "    num_classes = 4                           # 分类数量\n",
    "    img_size = 256                            # 输入图像尺寸\n",
    "    batch_size = 32                           # 批次大小\n",
    "    epochs = 20                               # 训练轮次\n",
    "    lr = 5e-5                                 # 初始学习率\n",
    "    seed = 42                                 # 随机种子 (保证复现性)\n",
    "    \n",
    "    # --- 硬件配置 ---\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    num_workers = 4 if os.name != 'nt' else 0 # Windows下建议设为0\n",
    "\n",
    "# 确保必要的目录存在\n",
    "Config.SAVE_DIR.mkdir(exist_ok=True, parents=True)\n",
    "print(f\"运行环境: {Config.device} | 模型架构: {Config.model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c8f020",
   "metadata": {},
   "source": [
    "#### 2. 工具函数 (数据清洗与可视化)\n",
    "包含数据MD5去重校验功能，以及训练过程的Loss/Accuracy曲线绘制函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd446815",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_and_clean_data(root_dir, backup_dir):\n",
    "    \"\"\"\n",
    "    通过计算MD5哈希值检测并移除重复图片，防止数据泄漏。\n",
    "    \"\"\"\n",
    "    root_dir = Path(root_dir)\n",
    "    backup_dir = Path(backup_dir)\n",
    "    \n",
    "    if not root_dir.exists():\n",
    "        print(f\"提示: 数据目录 {root_dir} 不存在，请检查路径。\")\n",
    "        return\n",
    "\n",
    "    backup_dir.mkdir(exist_ok=True)\n",
    "    unique_hashes = {}\n",
    "    duplicates_count = 0\n",
    "    \n",
    "    print(\"进行数据完整性校验...\")\n",
    "    image_exts = {'.jpg', '.jpeg', '.png', '.bmp', '.tif'}\n",
    "    file_list = [p for p in root_dir.rglob('*') if p.suffix.lower() in image_exts]\n",
    "    \n",
    "    for file_path in tqdm(file_list, desc=\"扫描文件\", leave=False):\n",
    "        try:\n",
    "            with open(file_path, \"rb\") as f:\n",
    "                file_hash = hashlib.md5(f.read()).hexdigest()\n",
    "            \n",
    "            if file_hash in unique_hashes:\n",
    "                # 发现重复，移动到备份目录\n",
    "                target_dir = backup_dir / file_path.parent.name\n",
    "                target_dir.mkdir(parents=True, exist_ok=True)\n",
    "                shutil.move(str(file_path), str(target_dir / file_path.name))\n",
    "                duplicates_count += 1\n",
    "            else:\n",
    "                unique_hashes[file_hash] = file_path\n",
    "        except Exception as e:\n",
    "            print(f\"读取错误: {file_path} - {e}\")\n",
    "\n",
    "    if duplicates_count > 0:\n",
    "        print(f\"移除了 {duplicates_count} 张重复图片至 {backup_dir}\")\n",
    "    else:\n",
    "        print(\"数据校验通过: 未发现重复样本。\")\n",
    "\n",
    "def plot_history(history, save_path=None):\n",
    "    \"\"\"\n",
    "    绘制训练过程中的 Loss 和 Accuracy 变化曲线。\n",
    "    \"\"\"\n",
    "    epochs = range(1, len(history['train_loss']) + 1)\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "    \n",
    "    # 绘制 Loss\n",
    "    axes[0].plot(epochs, history['train_loss'], 'b.-', label='Train')\n",
    "    axes[0].plot(epochs, history['val_loss'], 'r.-', label='Val')\n",
    "    axes[0].set_title('Loss Curve')\n",
    "    axes[0].set_xlabel('Epochs')\n",
    "    axes[0].set_ylabel('Loss')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True)\n",
    "    \n",
    "    # 绘制 Accuracy\n",
    "    axes[1].plot(epochs, history['train_acc'], 'b.-', label='Train')\n",
    "    axes[1].plot(epochs, history['val_acc'], 'r.-', label='Val')\n",
    "    axes[1].set_title('Accuracy Curve')\n",
    "    axes[1].set_xlabel('Epochs')\n",
    "    axes[1].set_ylabel('Accuracy (%)')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True)\n",
    "    \n",
    "    # 绘制 Learning Rate\n",
    "    axes[2].plot(epochs, history['lr'], 'g.-')\n",
    "    axes[2].set_title('Learning Rate Schedule')\n",
    "    axes[2].set_xlabel('Epochs')\n",
    "    axes[2].set_yscale('log')\n",
    "    axes[2].grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    if save_path:\n",
    "        plt.savefig(save_path)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "309ca7d3",
   "metadata": {},
   "source": [
    "#### 3. 数据预处理与加载\n",
    "构建Dataset与DataLoader。训练集采用强增强策略，验证/测试集仅进行标准化处理。数据集按 7.5:1.5:1.5 比例进行物理隔离划分。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d8f54912",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloaders(cfg):\n",
    "    # 训练集数据增强 (Augmentation)，包括随机裁剪、颜色抖动、仿射变换和水平翻转\n",
    "    train_tf = transforms.Compose([\n",
    "        transforms.RandomResizedCrop(cfg.img_size, scale=(0.8, 1.0)), \n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "        transforms.RandomAffine(degrees=10, translate=(0.05, 0.05)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    # 验证/测试集预处理 (Standardization)\n",
    "    val_tf = transforms.Compose([\n",
    "        transforms.Resize((cfg.img_size, cfg.img_size)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    # 加载数据集\n",
    "    if not cfg.DATA_DIR.exists():\n",
    "        raise FileNotFoundError(f\"数据目录不存在: {cfg.DATA_DIR}\")\n",
    "\n",
    "    # 使用两个实例分别应用不同的Transform\n",
    "    full_train_set = datasets.ImageFolder(cfg.DATA_DIR, transform=train_tf)\n",
    "    full_val_set = datasets.ImageFolder(cfg.DATA_DIR, transform=val_tf)\n",
    "    \n",
    "    # 确定划分索引 (使用固定种子确保每次运行测试集一致)\n",
    "    total_len = len(full_train_set)\n",
    "    indices = torch.randperm(total_len, generator=torch.Generator().manual_seed(cfg.seed)).tolist()\n",
    "    \n",
    "    train_len = int(0.7 * total_len)\n",
    "    val_len = int(0.15 * total_len)\n",
    "    \n",
    "    train_idx = indices[:train_len]\n",
    "    val_idx = indices[train_len : train_len + val_len]\n",
    "    test_idx = indices[train_len + val_len:]\n",
    "    \n",
    "    # 构建子集\n",
    "    train_ds = Subset(full_train_set, train_idx)\n",
    "    val_ds = Subset(full_val_set, val_idx)\n",
    "    test_ds = Subset(full_val_set, test_idx) # 测试集使用标准预处理\n",
    "    \n",
    "    # 构建DataLoader\n",
    "    loaders = {\n",
    "        'train': DataLoader(train_ds, batch_size=cfg.batch_size, shuffle=True, \n",
    "                          num_workers=cfg.num_workers, pin_memory=True),\n",
    "        'val': DataLoader(val_ds, batch_size=cfg.batch_size, shuffle=False, \n",
    "                        num_workers=cfg.num_workers, pin_memory=True),\n",
    "        'test': DataLoader(test_ds, batch_size=cfg.batch_size, shuffle=False, \n",
    "                         num_workers=cfg.num_workers, pin_memory=True)\n",
    "    }\n",
    "    \n",
    "    print(f\"数据加载完成 | 训练集: {len(train_ds)}, 验证集: {len(val_ds)}, 测试集: {len(test_ds)}\")\n",
    "    return loaders, len(full_train_set.classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "642bec25",
   "metadata": {},
   "source": [
    "#### 4. 模型构建与训练核心逻辑\n",
    "定义模型加载函数、单轮训练函数 (train_one_epoch) 和 评估函数 (evaluate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48a36a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(cfg):\n",
    "    \"\"\"加载预训练模型并修改分类头\"\"\"\n",
    "    print(f\"正在构建模型: {cfg.model_name}...\")\n",
    "    model = timm.create_model(\n",
    "        cfg.model_name, \n",
    "        pretrained=True, \n",
    "        num_classes=cfg.num_classes\n",
    "    )\n",
    "    return model.to(cfg.device)\n",
    "\n",
    "def train_one_epoch(model, loader, optimizer, criterion, scaler, cfg):\n",
    "    model.train()\n",
    "    total_loss, correct, total = 0, 0, 0\n",
    "    \n",
    "    loop = tqdm(loader, desc=\"[Train]\", leave=False)\n",
    "    for x, y in loop:\n",
    "        x, y = x.to(cfg.device), y.to(cfg.device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        # 混合精度训练\n",
    "        with torch.amp.autocast('cuda', enabled=(cfg.device=='cuda')):\n",
    "            pred = model(x)\n",
    "            loss = criterion(pred, y)\n",
    "            \n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 5.0) # 梯度裁剪\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        correct += (pred.argmax(1) == y).sum().item()\n",
    "        total += y.size(0)\n",
    "        \n",
    "        loop.set_postfix(loss=loss.item())\n",
    "        \n",
    "    return total_loss / len(loader), 100 * correct / total\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader, criterion, cfg):\n",
    "    model.eval()\n",
    "    total_loss, correct, total = 0, 0, 0\n",
    "    \n",
    "    for x, y in tqdm(loader, desc=\"[Eval]\", leave=False):\n",
    "        x, y = x.to(cfg.device), y.to(cfg.device)\n",
    "        with torch.amp.autocast('cuda', enabled=(cfg.device=='cuda')):\n",
    "            pred = model(x)\n",
    "            loss = criterion(pred, y)\n",
    "            \n",
    "        total_loss += loss.item()\n",
    "        correct += (pred.argmax(1) == y).sum().item()\n",
    "        total += y.size(0)\n",
    "        \n",
    "    return total_loss / len(loader), 100 * correct / total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0559d3e",
   "metadata": {},
   "source": [
    "#### 5. 执行模型训练\n",
    "测试时这个单元格可以跳过"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457d2332",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 数据准备\n",
    "check_and_clean_data(Config.DATA_DIR, Config.BACKUP_DIR)\n",
    "loaders, num_classes = get_dataloaders(Config)\n",
    "\n",
    "# 2. 模型与优化器初始化\n",
    "model = build_model(Config)\n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=Config.lr, weight_decay=0.05)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=Config.epochs)\n",
    "scaler = torch.amp.GradScaler('cuda', enabled=(Config.device=='cuda'))\n",
    "\n",
    "# 3. 训练循环\n",
    "history = {'train_loss': [], 'val_loss': [], 'train_acc': [], 'val_acc': [], 'lr': []}\n",
    "best_acc = 0.0\n",
    "\n",
    "print(f\"\\n开始训练 ({Config.epochs} epochs)...\")\n",
    "\n",
    "for epoch in range(1, Config.epochs + 1):\n",
    "    curr_lr = optimizer.param_groups[0]['lr']\n",
    "    \n",
    "    # 训练与验证\n",
    "    t_loss, t_acc = train_one_epoch(model, loaders['train'], optimizer, criterion, scaler, Config)\n",
    "    v_loss, v_acc = evaluate(model, loaders['val'], criterion, Config)\n",
    "    \n",
    "    scheduler.step()\n",
    "    \n",
    "    # 记录日志\n",
    "    history['train_loss'].append(t_loss)\n",
    "    history['val_loss'].append(v_loss)\n",
    "    history['train_acc'].append(t_acc)\n",
    "    history['val_acc'].append(v_acc)\n",
    "    history['lr'].append(curr_lr)\n",
    "    \n",
    "    # 保存最佳权重\n",
    "    save_msg = \"\"\n",
    "    if v_acc > best_acc:\n",
    "        best_acc = v_acc\n",
    "        torch.save(model.state_dict(), Config.SAVE_DIR / \"best_model.pth\")\n",
    "        save_msg = f\"最佳模型已保存 ({best_acc:.2f}%)\"\n",
    "        \n",
    "    print(f\"Epoch {epoch:02d} | LR: {curr_lr:.2e} | \"\n",
    "          f\"Train: {t_loss:.4f} / {t_acc:.2f}% | \"\n",
    "          f\"Val: {v_loss:.4f} / {v_acc:.2f}% {save_msg}\")\n",
    "\n",
    "print(f\"\\n训练结束，最高验证集准确率: {best_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c7c27b",
   "metadata": {},
   "source": [
    "#### 6. 结果可视化\n",
    "绘制并保存训练过程中的各项指标变化图。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e52dcd39",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history, save_path=Config.BASE_DIR / \"training_result.png\")\n",
    "print(f\"曲线图保存至: {Config.BASE_DIR / 'training_result.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e2b6c5",
   "metadata": {},
   "source": [
    "#### 7. 模型评估与测试\n",
    "本单元格将直接加载保存的最佳模型权重 (best_model.pth)，并在独立的测试集 (Test Set) 上进行推理，输出最终的评估指标。不需要重新进行训练。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6f035dea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在加载最佳权重: d:\\IT\\CODE\\JUPYTER\\ConvNeXt_V2\\checkpoints\\best_model.pth\n",
      "准备测试集数据...\n",
      "数据加载完成 | 训练集: 4940, 验证集: 1058, 测试集: 1060\n",
      "正在构建模型: swinv2_tiny_window16_256...\n",
      "\n",
      "inference...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 90.75%\n",
      "Test Loss:     0.2939\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "def run_inference_test():\n",
    "    weight_path = Config.SAVE_DIR / \"best_model.pth\"\n",
    "    \n",
    "    if not weight_path.exists():\n",
    "        print(f\"错误: 找不到权重文件 {weight_path}。请先运行上方训练代码或上传权重文件。\")\n",
    "        return\n",
    "\n",
    "    print(f\"正在加载最佳权重: {weight_path}\")\n",
    "    print(\"准备测试集数据...\")\n",
    "    \n",
    "    # 重新获取Loader (保证测试集划分与训练时一致)\n",
    "    loaders, _ = get_dataloaders(Config)\n",
    "    test_loader = loaders['test']\n",
    "    \n",
    "    # 初始化模型结构\n",
    "    model = build_model(Config)\n",
    "    # 加载权重\n",
    "    model.load_state_dict(torch.load(weight_path, map_location=Config.device))\n",
    "    \n",
    "    # 定义测试用的Loss\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    print(\"\\ninference...\")\n",
    "    test_loss, test_acc = evaluate(model, test_loader, criterion, Config)\n",
    "    \n",
    "    print(f\"Test Accuracy: {test_acc:.2f}%\")\n",
    "    print(f\"Test Loss:     {test_loss:.4f}\")\n",
    "\n",
    "# 执行推理\n",
    "if __name__ == \"__main__\":\n",
    "    run_inference_test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SV2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
